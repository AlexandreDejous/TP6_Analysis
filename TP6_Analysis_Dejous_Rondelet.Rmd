---
title: "TP6_Analysis_Dejous_Rondelet"
author: "Alexandre Dejous, Lucien Rondelet"
date: "5/17/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part A: Stationnary Analysis

## Question 1 and 2:

```{r}
setwd("C:/Users/Alexandre/Desktop/A2/Analyse/TP6_Analysis")
library("tseries")
data(USeconomic)
logGNP = as.vector(USeconomic[,2])
year = seq(1954,1987.75,0.25)
DATA = data.frame(year,logGNP)
plot(DATA$year, DATA$logGNP)
```

## Question 3

In a stationnary time serie, the ensemble mean and thetime average of a sample path are approximately equal.
For a strict stationnary time serie, all the observations are drawn from the same distribution, for a weak stationnary time serie, we expect only the observations to come from distributions with the same mean, variance and covariance.
In the plot we just drew, we visually assess that the samples values increase gradually on average on a significant number of samples, which should not happen if the samples were taken from the same distribution. We conclude that the time serie is not stationnary.

## Question 4

```{r}
#par(mfrow= c(2,1))
acf = acf(DATA$logGNP, 40)
pacf(DATA$logGNP,40)
```

It seems that tha ACF is steadily decreasing, which could indicate that logGNP follows a trend.

## Question 5:

```{r}
Box.test(DATA$logGNP)
```

The null hypothesis of this test stipulates that there is no auto-correlation between the values taken by our data.
However, since our p value is very close to 0, we reject this null hypothesis and have confirmation that our data follows and trend and is not just white noise.

## Question 6:

As is (not derived) our time series is not stationnary. It follows a trend and its samples are not taken from an iid distribution, as shown by the bow pierce test.


# PART B: Study of DiffGNP

## Question 1 and 2

```{r}
DiffGNP = diff(DATA$logGNP)
plot(DATA$year[1:length(DiffGNP)],DiffGNP,xlab="year", type ="l")
  
```

This time series represents the variation between two consecutive samples at time t (diff between t+1 and t).

## Question 3:

```{r}
t.test(DiffGNP)
```

This student test supports the following null hypothesis: the value of the mean is not equal to 0. In fact, the true mean should be a positive value, as shows the IC95, which is coherent with our previous results : the original time series follows a trend and this trend seems to be positive.

## Question 4:

```{r}
#par(mfrow= c(1,2))
acf(DiffGNP,40)
pacf(DiffGNP,40)
```

The ACF shows us that the q parameter of our arma model (applied to diff) might be 1 or 2, while the p parameter determined by the PACF might be 0 or 8. Our ARMA model (applied to diff) might be ARMA(0,1), ARMA(8,1), ARMA(0,2) or ARMA(8,2).

## Question 5:

```{r echo=FALSE}
#c(p,0,q)
print("---------------0,1---------------")
arima(DiffGNP,c(0,0,1))[5:6]
print("---------------8,1---------------")
arima(DiffGNP,c(8,0,1))[5:6]
print("---------------0,2---------------")
arima(DiffGNP,c(0,0,2))[5:6]
print("---------------8,2---------------")
arima(DiffGNP,c(8,0,2))[5:6]

```

The log likelihood gives us a measure of fitness for our model, the higher the better.
The AIC gives a measure of fitness and evaluates the simplicity of our model at the same time. The simplest and fittest model minimizes the AIC index.

If we only look at the log likelihood, it seems that the model ARMA(8,2) is the best. 
However, when comparing the two model with the AIC, the AIC indicates us that we should choose the model ARMA(0,2), because it has less parameters.

## Question 6:

If the residuals follow a trend, it means that our model could fit better the samples, if they don't, it means that our model perfectly fits the samples and is adapted.
We do the Box-Pierce test and Shapiro-Wilk test on the residuals of our ARMA model applied to DiffGNP for these coefficients : arma(0,1), arma(0,2) and arma(8,2).

The null hypothesis of the Box-Pierce test states that there is no autocorrelation in the data, while the null hypothesis of the Shapiro-Wilk test states that the samples came from a normally distributed population.

To ensure that our model has the best accuracy possible, we must make sure that the residuals don't follow a trend, and that they're normally distributed.

Thus, we need to check that the p-value of both test does not reject the null hypothesis (p> 0.05), we can also compare the models between them.

```{r echo=FALSE}
#little p rejects null hypothesis
model0_1 = arima(DiffGNP,c(0,0,1))$residual
model0_2 = arima(DiffGNP,c(0,0,2))$residual
model8_2 = arima(DiffGNP,c(8,0,2))$residual

print("---------------0,1---------------")
Box.test(model0_1)
shapiro.test(model0_1)
print("---------------0,2---------------")
Box.test(model0_2)
shapiro.test(model0_2)
print("---------------8,2---------------")
Box.test(model8_2)
shapiro.test(model8_2)

```

The box pierce test indicates that there is a higher suspicion for a trend for model (0,1) rather than the two other model, whose p-values are close to 1.
The shapiro wilk test indicates that the model (0,1) did not come from a normally distributed population, while for the two other models H0 still holds.

It would be useful to display the aurocorrelogram of the residuals, in order to see if the residuals of a model can themselves be described by another model.
Indeed, we obtain these results:

```{r}
par(mfrow= c(1,3))

acf(model0_1)

acf(model0_2)

acf(model8_2)


```

We cannot seem to make sense of a model for (0,2) and (8,2), however, the acf of the residuals of (0,1) indicates that these can be modeled by a MA(2).
The model (0,1) lacks accuracy, we can definetly direct our attention to the two other models.

Having to choose between (0,2) and (8,2), we decide to choose (8,2), because it seems like the most adapted model, we base our choice upon the results of the box-pierce test and the log-likelihood indicator.

# Part C: Predictions using ARMA

## Question 1 and 2:


```{r}
n <- 10
T=length(DiffGNP)
index <- 1:(T - n - 1)
res01 <- predict(arima(DiffGNP[index], c(0, 0, 1)), n)
res02 <- predict(arima(DiffGNP[index], c(0, 0, 2)), n)
res82 <- predict(arima(DiffGNP[index], c(8, 0, 2)), n)

plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(0,2)", t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred))
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred) + c(0,res02$se) * 1.96, lty = 2)
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred) - c(0,res02$se) * 1.96, lty = 2)
plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(8,2)",t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred), col = "red")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred) + c(0,res82$se) * 1.96, lty = 2, col = "red")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred) - c(0,res82$se) * 1.96, lty = 2, col = "red")
plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(0,1)",t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred), col = "green")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred) + c(0,res01$se) * 1.96, lty = 2, col = "green")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred) - c(0,res01$se) * 1.96, lty = 2, col = "green")
```

The models ARMA(0,1) and ARMA(0,2) are converging very fast, stabilizing around a certain value.
The model ARMA(8,2), on his part, is varying and at certain times its lagged difference (lag=1) is equal to the lagged difference of the original data.
It seems that the model (8,2) gives the best results.

## Question 3:

The model ARMA(8,2) applied on the data DiffGNP is equivalent to a model ARIMA(8,1,2) applied to the data logGNP. Indeed DiffGNP is the lagged difference with lag =1 of the data logGNP. 

# Part 4: ARIMA Model

```{r}
Diff2GNP = diff(DiffGNP)
plot(year[1:length(Diff2GNP)],Diff2GNP,type ="l")
t.test(Diff2GNP)
```

The Student test indicates that the mean is very close to 0 (-0.002 < mean < 0.0022). Even though the p-value is high, we will consider the underlying distribution centered.

```{r}
#par(mfrow= c(1,2))
acf(Diff2GNP, 40)
pacf(Diff2GNP,40)

```
 
With the ACF and PACF applied to Diff2GNP we obtain for p : 1, 3, 5, 7 and for q : 1.

```{r echo=FALSE}
#little p rejects null hypothesis
coeff = c(0,2,4,8)

print("---------------0,1---------------")
model0_1=arima(Diff2GNP,c(coeff[1],0,1))$residual
arima(Diff2GNP,c(coeff[1],0,1))[5:6]
Box.test(model0_1)
shapiro.test(model0_1)
print("---------------2,1---------------")
model2_1=arima(Diff2GNP,c(coeff[2],0,1))$residual
arima(Diff2GNP,c(coeff[2],0,1))[5:6]
Box.test(model2_1)
shapiro.test(model2_1)
print("---------------4,1---------------")
model4_1=arima(Diff2GNP,c(coeff[3],0,1))$residual
arima(Diff2GNP,c(coeff[3],0,1))[5:6]
Box.test(model4_1)
shapiro.test(model4_1)
print("---------------8,1---------------")
model8_1=arima(Diff2GNP,c(coeff[4],0,1))$residual
arima(Diff2GNP,c(coeff[4],0,1))[5:6]
Box.test(model8_1)
shapiro.test(model8_1)
```


These test results show that the models (8,1) and (4,1) are the best, however, for (8,1), despite its log_likelihood being 2 units higher than (4,1), (4,1) has a higher p-value regarding its normality test on its residuals.
We will continue the analysis with these two models.

```{r}

acf(model4_1)

acf(model8_1)


```

There doesnt seem to be any model we can build on top of the residuals of these two acf.
Now let's compare how the models look like when plot alongside the data.

```{r echo=FALSE}

n <- 10
T=length(logGNP)
index <- 1:(T - n)
res421 <- predict(arima(logGNP[index], c(4, 2, 1)), n)

plot(year[(T - 4 * n):T], logGNP[(T - 4 * n):T], main = "prevision ARIMA(4,2,1)", t = "l", col = "blue", xlab = "temps", ylab = "logGNP")
lines(year[(T - n):T], c(logGNP[T - n], res421$pred))
lines(year[(T - n):T], c(logGNP[T - n], res421$pred) + c(0,res421$se) * 1.96, lty = 2)
lines(year[(T - n):T], c(logGNP[T - n], res421$pred) - c(0,res421$se) * 1.96, lty = 2)

res821 <- predict(arima(logGNP[index], c(8, 2, 1)), n)

plot(year[(T - 4 * n):T], logGNP[(T - 4 * n):T], main = "prevision ARIMA(8,2,1)", t = "l", col = "blue", xlab = "temps", ylab = "logGNP")
lines(year[(T - n):T], c(logGNP[T - n], res821$pred))
lines(year[(T - n):T], c(logGNP[T - n], res821$pred) + c(0,res821$se) * 1.96, lty = 2)
lines(year[(T - n):T], c(logGNP[T - n], res821$pred) - c(0,res821$se) * 1.96, lty = 2)




```

The trend followed by the model (4,2,1) seem to best follow the samples than (8,2,1).
We think the best possible model for time series prediction on this data is an ARIMA(4,2,1) model.