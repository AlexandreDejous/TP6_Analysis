---
title: "TP6_Analysis_Dejous_Rondelet"
author: "Alexandre Dejous, Lucien Rondelet"
date: "5/17/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part A: Stationnary Analysis

## Question 1 and 2:

```{r}
setwd("D:/Documents/A2/Data/TP6_Analysis")
library(tseries)
data(USeconomic)
logGNP = as.vector(USeconomic[,2])
year = seq(1954,1987.75,0.25)
DATA = data.frame(year,logGNP)
plot(DATA$year, DATA$logGNP)
```

## Question 3

In a stationnary time serie, the ensemble mean and thetime average of a sample path are approximately equal.
For a strict stationnary time serie, all the observations are drawn from the same distribution, for a weak stationnary time serie, we expect only the observations to come from distributions with the same mean, variance and covariance.
In the plot we just drew, we visually assess that the samples values increase gradually on average on a significant number of samples, which should not happen if the samples were taken from the same distribution. We conclude that the time serie is not stationnary.

## Question 4

```{r}
par(mfrow= c(2,1))
acf = acf(DATA$logGNP, 40)
pacf(DATA$logGNP,40)
```

It seems that tha ACF is steadily decreasing, which could indicate that logGNP follows a trend.
The PACF indicates us that this trend can be modeled with an auto regressive model of order 1.

## Question 5:

```{r}
Box.test(DATA$logGNP)
```

The null hypothesis of this test stipulates that there is no auto-correlation between the values taken by our data.
However, since our p value is very close to 0, we reject this null hypothesis and have confirmation that our data follows and trend and is not just white noise.

## Question 6:

As is (not derived) our time series is not stationnary. It follows a trend and its samples are not taken from an iid distribution, as shown by the bow pierce test.


# PART B: Study of DiffGNP

## Question 1 and 2

```{r}
DiffGNP = diff(DATA$logGNP)
plot(DATA$year[1:length(DiffGNP)],DiffGNP,xlab="year", type ="l")
  
```

This time series represents the variation between two consecutive samples at time t (diff between t+1 and t).

## Question 3:

```{r}
t.test(DiffGNP)
```

This student test supports the following null hypothesis: the value of the mean is not equal to 0. In fact, the true mean should be a positive value, as shows the IC95, which is coherent with our previous results : the original time series follows a trend and this trend seems to be positive.

## Question 4:

```{r}
par(mfrow= c(1,2))
acf(DiffGNP)
pacf(DiffGNP)
```

The ACF shows us that the q parameter of our arma model might be 1 or 2, while the p parameter determined by the PACF might be 0 or 8. Our ARMA model might be ARMA(0,1), ARMA(8,1), ARMA(0,2) or ARMA(8,2).

## Question 5:

```{r echo=FALSE}
#c(p,0,q)
print("---------------0,1---------------")
arima(DiffGNP,c(0,0,1))[5:6]
print("---------------8,1---------------")
arima(DiffGNP,c(8,0,1))[5:6]
print("---------------0,2---------------")
arima(DiffGNP,c(0,0,2))[5:6]
print("---------------8,2---------------")
arima(DiffGNP,c(8,0,2))[5:6]

```

The log likelihood gives us a measure of fitness for our model, the higher the better.
The AIC gives a measure of fitness and evaluates the simplicity of our model at the same time. The simplest and fittest model minimizes the AIC index.

If we only look at the log likelihood, it seems that the model ARMA(8,2) is the best. 
However, when comparing the two model with the AIC, the AIC indicates us that we should choose the model ARMA(0,2), because it has less parameters.

## Question 6:

If the residuals follow a trend, it means that our model could fit better the samples, if they don't, it means that our model perfectly fits the samples and is adapted.
We do the Box-Pierce test and Shapiro-Wilk test on the residuals of our ARMA model applied to DiffGNP for these coefficients : arma(0,1), arma(0,2) and arma(8,2).

The null hypothesis of the Box-Pierce test states that there is no autocorrelation in the data, while the null hypothesis of the Shapiro-Wilk test states that the samples came from a normally distributed population.

To ensure that our model has the best accuracy possible, we must make sure that the residuals don't follow a trend, and that they're normally distributed.

Thus, we need to check that the p-value of both test does not reject the null hypothesis (p> 0.05), we can also compare the models between them.

```{r echo=FALSE}
#little p rejects null hypothesis
model0_1 = arima(DiffGNP,c(0,0,1))$residual
model0_2 = arima(DiffGNP,c(0,0,2))$residual
model8_2 = arima(DiffGNP,c(8,0,2))$residual

print("---------------0,1---------------")
Box.test(model0_1)
shapiro.test(model0_1)
print("---------------0,2---------------")
Box.test(model0_2)
shapiro.test(model0_2)
print("---------------8,2---------------")
Box.test(model8_2)
shapiro.test(model8_2)

```

The box pierce test indicates that there is a higher suspicion for a trend for model (0,1) rather than the two other model, whose p-values are close to 1.
The shapiro wilk test indicates that the model (0,1) did not come from a normally distributed population, while for the two other models H0 still holds.

It would be useful to display the aurocorrelogram of the residuals, in order to see if the residuals of a model can themselves be described by another model.
Indeed, we obtain these results:

```{r}
par(mfrow= c(1,3))

acf(model0_1)

acf(model0_2)

acf(model8_2)


```

We cannot seem to make sense of a model for (0,2) and (8,2), however, the acf of the residuals of (0,1) indicates that these can be modeled by a MA(2).
The model (0,1) lacks accuracy, we can definetly direct our attention to the two other models.

Having to choose between (0,2) and (8,2), we decide to choose (8,2), because it seems like the most adapted model, we base our choice upon the results of the box-pierce test and the log-likelihood indicator.

# Part C: Predictions using ARMA

## Question 1 and 2:


```{r}
n <- 10
T=length(DiffGNP)
index <- 1:(T - n - 1)
res01 <- predict(arima(DiffGNP[index], c(0, 0, 1)), n)
res02 <- predict(arima(DiffGNP[index], c(0, 0, 2)), n)
res82 <- predict(arima(DiffGNP[index], c(8, 0, 2)), n)

plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(0,2)", t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred))
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred) + c(0,res02$se) * 1.96, lty = 2)
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res02$pred) - c(0,res02$se) * 1.96, lty = 2)
plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(8,2)",t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred), col = "red")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred) + c(0,res82$se) * 1.96, lty = 2, col = "red")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res82$pred) - c(0,res82$se) * 1.96, lty = 2, col = "red")
plot(year[(T - 4 * n):T], DiffGNP[(T - 4 * n):T - 1], main = "prevision ARMA(0,1)",t = "l", col = "blue", xlab = "temps", ylab = "diff GNP")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred), col = "green")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred) + c(0,res01$se) * 1.96, lty = 2, col = "green")
lines(year[(T - n):T], c(DiffGNP[T - n - 1], res01$pred) - c(0,res01$se) * 1.96, lty = 2, col = "green")
```

The models ARMA(0,1) and ARMA(0,2) are converging very fast, stabilizing around a certain value.
The model ARMA(8,2), on his part, is varying and at certain times its lagged difference (lag=1) is equal to the lagged difference of the original data.
It seems that the model (8,2) gives the best results.

## Question 3:

The model ARMA(8,2) applied on the data DiffGNP is equivalent to a model ARIMA(8,1,2) applied to the data logGNP. Indeed DiffGNP is the lagged difference with lag =1 of the data logGNP.

# Part 4:

## Question



```{r}
Diff2GNP = diff(DiffGNP)
plot(year[1:length(Diff2GNP)],Diff2GNP)
```
